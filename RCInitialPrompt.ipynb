{"cells":[{"cell_type":"markdown","source":["## REVISION CHAIN ANALYSIS\n","\n","This notebook evaluates student learning processes by analyzing their iterative  interactions with AI (revision chains) rather than just the final submission. It generates structured educational feedback based on rubric criteria."],"metadata":{"id":"jc8vdJObajmW"},"id":"jc8vdJObajmW"},{"cell_type":"markdown","id":"70a77877-692a-4947-b191-7cf28ad206f6","metadata":{"id":"70a77877-692a-4947-b191-7cf28ad206f6"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":40,"id":"99c87615-2575-48f4-8096-50dbe1210f07","metadata":{"id":"99c87615-2575-48f4-8096-50dbe1210f07","executionInfo":{"status":"ok","timestamp":1763851903826,"user_tz":-660,"elapsed":4,"user":{"displayName":"Steven C","userId":"06143743222157822721"}}},"outputs":[],"source":["# import required libraries\n","import json\n","from typing import Optional\n","from google import genai\n","from google.genai import types\n","from google.colab import userdata\n","from collections import Counter"]},{"cell_type":"markdown","id":"c1c7560b-8017-44fd-8711-0c4fcd7d3007","metadata":{"id":"c1c7560b-8017-44fd-8711-0c4fcd7d3007"},"source":["### CONFIGURATION"]},{"cell_type":"code","execution_count":41,"id":"0c216315-ee48-42aa-86d8-c85cd5824ac0","metadata":{"id":"0c216315-ee48-42aa-86d8-c85cd5824ac0","executionInfo":{"status":"ok","timestamp":1763851904602,"user_tz":-660,"elapsed":771,"user":{"displayName":"Steven C","userId":"06143743222157822721"}}},"outputs":[],"source":["# Initialize Gemini API client\n","client = genai.Client(api_key=userdata.get(\"GOOGLE_API_KEY\"))\n","GEMINI_MODEL = \"gemini-2.5-flash-lite\""]},{"cell_type":"markdown","id":"0290a465-90a2-4caf-a657-ba5060c38504","metadata":{"id":"0290a465-90a2-4caf-a657-ba5060c38504"},"source":["### PROMPT TEMPLATES"]},{"cell_type":"code","execution_count":42,"id":"0a368af5-02d2-45fe-b600-80582de6002c","metadata":{"id":"0a368af5-02d2-45fe-b600-80582de6002c","executionInfo":{"status":"ok","timestamp":1763851904610,"user_tz":-660,"elapsed":4,"user":{"displayName":"Steven C","userId":"06143743222157822721"}}},"outputs":[],"source":["SYSTEM_PROMPT = \"\"\"\n","You are an expert educational feedback generator trained to provide credible, educator-trusted feedback on student work.\n","\n","⚠️ CRITICAL INSTRUCTION ⚠️\n","You are evaluating ONLY what the STUDENT typed/asked in their prompts.\n","You are NOT evaluating the AI assistant's responses or interpretations.\n","\n","In the conversation:\n","- \"User:\" = STUDENT (the person being graded)\n","- \"AI:\" = Assistant (NOT being graded - ignore quality of these responses)\n","\n","The STUDENT is responsible ONLY for their own typed prompts.\n","The STUDENT is NOT responsible for what the AI said or how the AI interpreted things.\n","\n","Example of CORRECT analysis:\n","✓ \"The student typed 'pants' then continued asking about plants\"\n","✗ \"The student self-corrected from 'pants' to 'plants'\" ← WRONG! The AI corrected it, not the student\n","\n","Prioritize:\n","- Constructive feedback about what the STUDENT actually typed and asked\n","- Evidence of the STUDENT's question evolution and learning behaviors\n","- The STUDENT's ability to demonstrate active learning through their prompts\n","- Objectivity, professionalism, and clarity in communication\n","- Outputs that educators can directly adopt or edit\n","\"\"\"\n","\n","STRUCTURED_OUTPUT_PROMPT = \"\"\"\n","Format your output EXACTLY as shown below with clear section headers:\n","\n","═════════════════════════════════════════════════════════\n","RUBRIC SCORES\n","═════════════════════════════════════════════════════════\n","Criterion 1 (Inquiry Depth & Progression): [0–5] - [One sentence about student's question evolution]\n","Criterion 2 (Active Learning Behaviors): [0–5] - [One sentence about student's engagement behaviors]\n","Criterion 3 (Prompt Clarity & Intentionality): [0–5] - [One sentence about student's communication clarity]\n","Criterion 4 (Persistence & Goal-Directedness): [0–5] - [One sentence about student's sustained focus]\n","\n","═════════════════════════════════════════════════════════\n","LEARNING BEHAVIOR ANALYSIS\n","═════════════════════════════════════════════════════════\n","\n","→ Inquiry Pattern Analysis:\n","[2-3 sentences analyzing the student's questioning strategy - did they go deep or stay surface-level?]\n","\n","→ Active vs. Passive Learning:\n","[2-3 sentences evaluating whether the student showed active learning behaviors or passive answer-seeking]\n","\n","→ Communication Effectiveness:\n","[2-3 sentences about how clearly and purposefully the student communicated their learning needs]\n","\n","→ Learning Goal Achievement:\n","[2-3 sentences about whether the student's approach demonstrates genuine learning vs. answer extraction]\n","\n","═════════════════════════════════════════════════════════\n","LEARNING PROCESS SUMMARY\n","═════════════════════════════════════════════════════════\n","\n","[Opening Paragraph - 3-4 sentences]\n","Overall assessment: Is this student using the LLM to genuinely learn, or just to extract answers?\n","\n","[Evidence Paragraph - 4-5 sentences]\n","Specific evidence from the student's prompts that demonstrates their learning approach\n","\n","[Recommendations Paragraph - 2-3 sentences]\n","Concrete suggestions for how the student could improve their LLM-assisted learning\n","\"\"\"\n","\n","def build_feedback_prompt(domain, assignment_prompt, rubric_text, submission, criterion_names, prompt_chain):\n","    \"\"\"\n","    Constructs the complete prompt for generating educational feedback focused on learning behaviors.\n","    \"\"\"\n","    # Extract only student prompts for emphasis\n","    student_prompts_only = []\n","    for turn in prompt_chain:\n","        if turn.startswith(\"User:\"):\n","            student_prompts_only.append(turn[6:].strip())\n","\n","    student_prompts_list = \"\\n\".join([f\"  {i+1}. \\\"{prompt}\\\"\" for i, prompt in enumerate(student_prompts_only)])\n","\n","    return f\"\"\"\n","{SYSTEM_PROMPT}\n","\n","TASK CONTEXT:\n","You are assessing a STUDENT's inquiry and learning process from the domain: **{domain}**.\n","The assignment prompt is:\n","\\\"\\\"\\\"{assignment_prompt}\\\"\\\"\\\"\n","\n","ASSESSMENT RUBRIC:\n","{rubric_text}\n","\n","═══════════════════════════════════════════════════════════\n","STUDENT'S PROMPTS (What the student actually typed):\n","═══════════════════════════════════════════════════════════\n","{student_prompts_list}\n","\n","═══════════════════════════════════════════════════════════\n","FULL CONVERSATION (For context only):\n","═══════════════════════════════════════════════════════════\n","{json.dumps(prompt_chain, indent=2)}\n","\n","⚠️ REMEMBER:\n","- Evaluate ONLY what the student typed (User: lines)\n","- AI responses are context only\n","- Do NOT credit the student for what the AI said or inferred\n","\n","FINAL SUBMISSION (For Context):\n","\\\"\\\"\\\"{submission}\\\"\\\"\\\"\n","\n","EVALUATION INSTRUCTIONS:\n","\n","1. **PRIMARY FOCUS**: Analyze ONLY the student's typed prompts (listed above).\n","\n","2. Evaluate the STUDENT based on their prompts:\n","   ✓ What topics did the STUDENT choose to ask about?\n","   ✓ How did the STUDENT's questions change over time?\n","   ✓ Did the STUDENT ask follow-up questions on topics?\n","   ✓ Did the STUDENT's prompts become more specific or stay general?\n","   ✓ Did the STUDENT ask \"why/how\" questions or request explanations?\n","\n","3. ⚠️ COMMON MISTAKES TO AVOID:\n","   ✗ Do NOT say \"the student self-corrected\" when the AI corrected something\n","   ✗ Do NOT credit the student for the AI's interpretations\n","   ✗ Do NOT evaluate the quality of AI responses\n","   ✗ Do NOT praise \"the student's ability to clarify\" when the AI asked for clarification\n","\n","   ONLY evaluate what the student actually typed.\n","\n","4. Examples of CORRECT analysis:\n","   ✓ \"The student typed 'pants' as their prompt\" (factual)\n","   ✓ \"The student continued asking about plant topics\" (observable)\n","   ✗ \"The student corrected their typo\" (the AI corrected it, not the student)\n","   ✗ \"The student clarified their intent\" (the student just moved on to the next topic)\n","\n","5. Focus on observable learning behaviors:\n","   - Progression from broad to specific (or vice versa)\n","   - Use of question words (why, how, what, when)\n","   - Requests for examples, explanations, or clarifications\n","   - Topic persistence (staying on one subject) vs exploration (jumping around)\n","   - Complexity or simplicity of prompts\n","\n","6. Provide credible, educationally meaningful feedback about the STUDENT's questioning behavior.\n","\n","7. Maintain structured outputs with clear focus on what the STUDENT did.\n","\n","{STRUCTURED_OUTPUT_PROMPT}\n","\"\"\""]},{"cell_type":"markdown","id":"626282ef-4182-4458-b4fe-911cc1213c62","metadata":{"id":"626282ef-4182-4458-b4fe-911cc1213c62"},"source":["### GENERATION FUNCTION"]},{"cell_type":"code","execution_count":43,"id":"05b354f3-ce9a-4bec-acb0-856777a4b390","metadata":{"id":"05b354f3-ce9a-4bec-acb0-856777a4b390","executionInfo":{"status":"ok","timestamp":1763851904623,"user_tz":-660,"elapsed":3,"user":{"displayName":"Steven C","userId":"06143743222157822721"}}},"outputs":[],"source":["# core function that generates eductaional feedback based on constructed prompt\n","def gen_text(prompt_str: str, max_new_tokens=512, temperature: Optional[float] = None):\n","    cfg = types.GenerateContentConfig(\n","        temperature=temperature if temperature is not None else 0.5,\n","        max_output_tokens=max_new_tokens,\n","    )\n","    resp = client.models.generate_content(\n","        model=GEMINI_MODEL,\n","        contents=prompt_str,\n","        config=cfg,\n","    )\n","    return (resp.text or \"\").strip()"]},{"cell_type":"markdown","id":"f999a7cc-78f6-4e59-a202-12f5b1260aba","metadata":{"id":"f999a7cc-78f6-4e59-a202-12f5b1260aba"},"source":["### Rubric Formatting"]},{"cell_type":"code","execution_count":44,"id":"ef823ab1-64a9-4228-90db-afa43ce39a8f","metadata":{"id":"ef823ab1-64a9-4228-90db-afa43ce39a8f","executionInfo":{"status":"ok","timestamp":1763851904626,"user_tz":-660,"elapsed":2,"user":{"displayName":"Steven C","userId":"06143743222157822721"}}},"outputs":[],"source":["# converts a rubic dictionary into a human readable form\n","def format_rubric(r: dict):\n","    formatted, names = [], []\n","    formatted.append(f\"Rubric ID: {r.get('rubric_id', 'N/A')}\\n\")\n","    formatted.append(\"Criteria:\\n\")\n","    for item in r.get('criteria', []):\n","        nm = item.get('name', 'Criterion')\n","        names.append(nm)\n","        formatted.append(f\"Criterion: {item.get('criterion_id','')}\\nName: {nm}\\nDescription: {item.get('description','')}\\nPerformance Descriptors:\\n\")\n","        for key, val in item.get('performance_descriptors', {}).items():\n","            formatted.append(f\"  - {key}: {val}\\n\")\n","    return \"\".join(formatted), names\n"]},{"cell_type":"markdown","id":"6bb0f0a9-a25d-4ed8-864e-0ca9386c19c0","metadata":{"id":"6bb0f0a9-a25d-4ed8-864e-0ca9386c19c0"},"source":["### MAIN LOOP"]},{"cell_type":"code","execution_count":45,"id":"d2b8ff9a-202a-40a3-b99a-16efad48c628","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2b8ff9a-202a-40a3-b99a-16efad48c628","executionInfo":{"status":"ok","timestamp":1763851908476,"user_tz":-660,"elapsed":3848,"user":{"displayName":"Steven C","userId":"06143743222157822721"}},"outputId":"0e63f9ef-6cae-4ad2-e6fe-ee9446662d45"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================= Processing RCTestingSample =================\n","\n","--- SUBMISSION 1 (True Label: Hybrid) ---\n","\n","--- EDUCATIONAL FEEDBACK ---\n","\n","It looks like you're ready to get started! I'll provide the feedback based on the student's prompts.\n","\n","═════════════════════════════════════════════════════════\n","RUBRIC SCORES\n","═════════════════════════════════════════════════════════\n","Criterion 1 (Inquiry Depth & Progression): 3 - The student moved from general requests for information to specific topics, but did not ask follow-up questions to deepen understanding.\n","Criterion 2 (Active Learning Behaviors): 2 - The student primarily provided keywords or general requests, with no explicit requests for clarification, examples, or explanations.\n","Criterion 3 (Prompt Clarity & Intentionality): 3 - The student's prompts were generally understandable, but often vague, requiring the AI to infer intent.\n","Criterion 4 (Persistence & Goal-Directedness): 3 - The student showed some focus on a topic (science/biology) but frequently switched subjects after only a few prompts.\n","\n","═════════════════════════════════════════════════════════\n","LEARNING BEHAVIOR ANALYSIS\n","═════════════════════════════════════════════════════════\n","\n","→ Inquiry Pattern Analysis:\n","The student's inquiry began with very broad requests (\"tell me something,\" \"fun fact\") and then moved to general topic areas (\"science,\" \"bio\"). While there was a progression towards more specific subjects, the student did not ask follow-up questions to explore these topics in greater depth. The sequence shows a desire to gather discrete facts rather than build a cohesive understanding.\n","\n","→ Active vs. Passive Learning:\n","The student's prompts indicate a largely passive learning approach. They provided keywords or general prompts and accepted the information provided without asking for further elaboration, examples, or clarification. There is no evidence of the student actively engaging with the material by questioning its implications or seeking deeper explanations.\n","\n","→ Communication Effectiveness:\n","The student's prompts were generally understandable but lacked specificity and clear learning intent. Initial prompts were very open-ended, and later prompts were single keywords or slightly misspelled terms. This required the AI to make assumptions about the student's desired learning path, rather than the student actively guiding the conversation with precise questions.\n","\n","→ Learning Goal Achievement:\n","The student's approach suggests a goal of collecting interesting facts rather than achieving a deep understanding of any particular topic. The rapid topic switching and lack of follow-up questions indicate a superficial engagement with the information presented. While the student explored several distinct pieces of information, this does not demonstrate strategic knowledge acquisition.\n","\n","═════════════════════════════════════════════════════════\n","LEARNING PROCESS SUMMARY\n","═════════════════════════════════════════════════════════\n","\n","This student appears to be using the LLM primarily as a source for interesting, discrete pieces of information rather than as a tool for in-depth learning. The prompts suggest a desire for novelty and quick facts, with limited evidence of a structured approach to building knowledge or understanding complex concepts. The interaction leans towards answer extraction over genuine inquiry and exploration.\n","\n","The student's prompts show a progression from very general requests (\"tell me something\") to more specific topics like \"science\" and \"bio.\" However, this progression is not sustained; the student then typed \"pants,\" which the AI interpreted as a potential typo for \"plants,\" and then moved to \"cranvorous\" and \"sundew.\" Crucially, at no point did the student ask clarifying questions, request examples, or delve deeper into any of the topics presented.\n","\n","To enhance their learning process, the student should focus on asking more specific and probing questions, such as \"how does X work?\" or \"why is Y important?\". Encouraging the student to ask follow-up questions after receiving information would foster deeper engagement and demonstrate a more active learning approach. Additionally, practicing clearer spelling and more descriptive prompts would improve communication and guide the AI more effectively.\n"]}],"source":["if __name__ == \"__main__\":\n","    # list of dataset files to process\n","    DATASETS = [\"RCTestingSample\"]  # replace with your actual dataset names if needed\n","    MAX_EXAMPLES = 3\n","\n","    for dataset in DATASETS:\n","        # load the test data from JSON file\n","        with open(\"RCTestingSample.json\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","\n","        # extract and format the rubic for this dataset\n","        rubric_text, criterion_names = format_rubric(data['rubric'])\n","\n","        print(f\"\\n================= Processing {dataset} =================\")\n","\n","        for i, submission in enumerate(data['submissions'], 1):\n","            # extract submission components\n","            submission_text = submission['final_submission']\n","            prompt_chain = submission.get(\"prompt_chain\", [])\n","            label_type = submission.get(\"label_type\", \"Unknown\")\n","\n","            # --- Generate structured, credible feedback ---\n","            # build the complete prompt withh all the content\n","            feedback_prompt = build_feedback_prompt(\n","                domain=data['domain'],\n","                assignment_prompt=data.get(\"prompt\", \"Analyze student submission\"),\n","                rubric_text=rubric_text,\n","                submission=submission_text,\n","                criterion_names=criterion_names,\n","                prompt_chain=prompt_chain\n","            )\n","\n","            # call LLM API to generate feedback\n","            feedback_response = gen_text(feedback_prompt, max_new_tokens=1024, temperature=0.5)\n","\n","            # Print results\n","            print(f\"\\n--- SUBMISSION {i} (True Label: {label_type}) ---\")\n","            print(\"\\n--- EDUCATIONAL FEEDBACK ---\\n\")\n","            print(feedback_response)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}