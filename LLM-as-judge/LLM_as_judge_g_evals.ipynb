{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c08a862406b49b89ee544abfe86b80e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_dccd97c43f24438b97f5eb6d6086ec1f",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFeedback Quality [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)…\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Feedback Quality [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)…</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "dccd97c43f24438b97f5eb6d6086ec1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZs_FPTiZU-6",
        "outputId": "1095a6c3-be56-4bb1-8c11-d5902181ee4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepeval\n",
            "  Downloading deepeval-3.7.5-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from deepeval) (3.13.2)\n",
            "Collecting click<8.3.0,>=8.0.0 (from deepeval)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.76.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from deepeval) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.6.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.9.0)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.37.0)\n",
            "Collecting portalocker (from deepeval)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting posthog<6.0.0,>=5.4.0 (from deepeval)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.12.3)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.12.0)\n",
            "Collecting pyfiglet (from deepeval)\n",
            "  Downloading pyfiglet-1.0.4-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from deepeval) (8.4.2)\n",
            "Collecting pytest-asyncio (from deepeval)\n",
            "  Downloading pytest_asyncio-1.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pytest-repeat (from deepeval)\n",
            "  Downloading pytest_repeat-0.9.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pytest-rerunfailures (from deepeval)\n",
            "  Downloading pytest_rerunfailures-16.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pytest-xdist (from deepeval)\n",
            "  Downloading pytest_xdist-3.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.2.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.32.4)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.6.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (13.9.4)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.47.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from deepeval) (75.2.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.9.0)\n",
            "Requirement already satisfied: tenacity<=10.0.0,>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (9.1.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (4.67.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9 in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.20.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.45.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.67.1->deepeval) (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval)\n",
            "  Downloading opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.39.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (5.29.5)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b0 (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (2.9.0.post0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=5.4.0->deepeval)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2025.11.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.6.0->deepeval) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.6.0->deepeval) (2.19.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->deepeval) (3.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai->deepeval) (4.12.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai->deepeval) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai->deepeval) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai->deepeval) (1.3.1)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->deepeval) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from pytest->deepeval) (25.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->deepeval) (1.6.0)\n",
            "Collecting execnet>=2.1 (from pytest-xdist->deepeval)\n",
            "  Downloading execnet-2.1.2-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai->deepeval) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->deepeval) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.6.0->deepeval) (0.1.2)\n",
            "Downloading deepeval-3.7.5-py3-none-any.whl (787 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m787.3/787.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading pyfiglet-1.0.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-1.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading pytest_repeat-0.9.4-py3-none-any.whl (4.2 kB)\n",
            "Downloading pytest_rerunfailures-16.1-py3-none-any.whl (14 kB)\n",
            "Downloading pytest_xdist-3.8.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading execnet-2.1.2-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyfiglet, portalocker, opentelemetry-proto, execnet, click, backoff, pytest-xdist, pytest-rerunfailures, pytest-repeat, pytest-asyncio, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, deepeval\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.1\n",
            "    Uninstalling click-8.3.1:\n",
            "      Successfully uninstalled click-8.3.1\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 click-8.2.1 deepeval-3.7.5 execnet-2.1.2 opentelemetry-api-1.39.0 opentelemetry-exporter-otlp-proto-common-1.39.0 opentelemetry-exporter-otlp-proto-grpc-1.39.0 opentelemetry-proto-1.39.0 opentelemetry-sdk-1.39.0 opentelemetry-semantic-conventions-0.60b0 portalocker-3.2.0 posthog-5.4.0 pyfiglet-1.0.4 pytest-asyncio-1.3.0 pytest-repeat-0.9.4 pytest-rerunfailures-16.1 pytest-xdist-3.8.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "# install libraries required\n",
        "!pip install deepeval         # evaluation framework\n",
        "# !pip install pydantic-ai      # the LLM judge interface\n",
        "!pip install openai           # used as the judge model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "import json\n",
        "import os\n",
        "# from google import genai\n",
        "from google.colab import userdata\n",
        "# from pydantic import BaseModel, Field\n",
        "# from pydantic_evals.evaluators import Evaluator, EvaluationResult\n",
        "# from pydantic_ai import Agent\n",
        "\n",
        "# DeepEval imports for G-Eval\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCase, LLMTestCaseParams"
      ],
      "metadata": {
        "id": "jLQ_9_nwaEMs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90f878f4"
      },
      "source": [
        "# import pydantic_evals.evaluators\n",
        "# print(dir(pydantic_evals.evaluators))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32713b9b",
        "outputId": "07196548-e4ca-45e7-e932-11b83ccfcfed"
      },
      "source": [
        "# import pydantic_evals\n",
        "# print(dir(pydantic_evals))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Case', 'Dataset', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_utils', 'dataset', 'evaluators', 'increment_eval_metric', 'otel', 'reporting', 'set_eval_attribute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load input files\n",
        "\n",
        "# load rubric, submission and prompt\n",
        "with open(\"rub_it_0002.json\", \"r\") as f:\n",
        "    rubric_data = json.load(f)\n",
        "\n",
        "# load llm feedback\n",
        "with open(\"FG_Feedback_Results_with_Scores.json\", \"r\") as f:\n",
        "    feedback_data = json.load(f)\n"
      ],
      "metadata": {
        "id": "8chZbtJrZmN4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract rubric, submission, and feedback\n",
        "rubric = rubric_data[\"rubric\"][\"criteria\"]\n",
        "student_submission = rubric_data[\"submissions\"][0][\"final_submission\"]\n",
        "llm_feedback = feedback_data[0][\"feedback\"]\n"
      ],
      "metadata": {
        "id": "QMNJV3Uma8lg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rubric: \", rubric, \"\\n\")\n",
        "print(\"Submission: \", student_submission, \"\\n\")\n",
        "print(\"Feedback: \", llm_feedback, \"\\n\")"
      ],
      "metadata": {
        "id": "TVAHv4gWbNp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231fec4e-84b7-4b76-e9ae-0735ed2e732c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rubric:  [{'criterion_id': 'c1', 'name': 'Conceptual Understanding', 'description': 'Demonstrates deep understanding of the topic and related concepts.', 'performance_descriptors': {'excellent': 'Shows precise and in-depth understanding with clear theoretical support.', 'good': 'Understands key concepts well with minor gaps.', 'average': 'Adequate explanation with some inaccuracies.', 'needs_improvement': 'Surface-level or confused understanding.', 'poor': 'Major misunderstandings or misrepresentations.'}, 'weight': 15}, {'criterion_id': 'c2', 'name': 'Application to Real-World Scenarios', 'description': 'Applies concepts meaningfully to examples or scenarios.', 'performance_descriptors': {'excellent': 'Uses insightful and relevant examples clearly linked to concepts.', 'good': 'Examples mostly appropriate with some gaps in explanation.', 'average': 'Examples present but superficial or unclear connections.', 'needs_improvement': 'Vague or generic examples.', 'poor': 'No real-world application.'}, 'weight': 20}, {'criterion_id': 'c3', 'name': 'Critical Evaluation', 'description': 'Analyzes challenges, limitations, or trade-offs thoughtfully.', 'performance_descriptors': {'excellent': 'Provides insightful, research-backed critical evaluation.', 'good': 'Reasonable evaluation but less depth.', 'average': 'Mentions challenges superficially.', 'needs_improvement': 'Unclear or irrelevant critique.', 'poor': 'No critical evaluation.'}, 'weight': 40}, {'criterion_id': 'c4', 'name': 'Structure and Academic Writing', 'description': 'Quality of writing, flow, citations, and tone.', 'performance_descriptors': {'excellent': 'Clear, coherent, formal tone, flawless grammar, and proper referencing.', 'good': 'Mostly clear and professional; minor issues.', 'average': 'Organization present but inconsistent.', 'needs_improvement': 'Structure or tone hinder clarity; referencing weak.', 'poor': 'Disorganized or informal writing.'}, 'weight': 25}] \n",
            "\n",
            "Submission:  Data breaches continue to affect organisations across industries as systems move online and attackers find new ways to exploit technical and human weaknesses. A large number of these incidents still involve misuse of valid credentials that have been stolen, guessed, or reused across different services. Because of this, multi-factor authentication (MFA) has become a key control for strengthening identity verification. Instead of trusting a password on its own, MFA requires users to prove their identity with two or more different factors, such as a password and a temporary code or biometric check. This essay evaluates how effective MFA is in preventing data breaches, compares common MFA methods and their usability–security trade-offs, and analyses how MFA fits into broader cybersecurity strategies. The argument presented is that MFA is highly effective at blocking many credential-based attacks, but its real-world value depends on the specific methods organisations choose, how consistently they enforce them, and whether usability, fallback mechanisms, and human factors are addressed as part of a wider defence-in-depth approach. Multi-factor authentication reduces the impact of credential compromise by adding additional barriers for attackers. In a single-factor model, a password functions as a single point of failure. If an attacker can obtain it through phishing, malware, or a leaked password database, they can usually log in without further resistance. Breach reports regularly show that such credential-based attacks are one of the most common initial intrusion vectors. By contrast, MFA combines something the user knows with something they have or something they are, meaning an attacker must also control a device—such as a phone, authenticator app, or hardware token—or successfully imitate a biometric factor. From a practical perspective, this layered approach blocks many broad, low-effort attacks. Stolen passwords from one service are less useful if another service requires MFA, because the attacker would also need access to the second factor. Basic phishing emails are also less effective when the password alone no longer grants access. However, the protection provided by MFA is not absolute. Attackers have adapted by targeting the additional factor itself, such as through SIM swapping to intercept text messages or conducting MFA fatigue attacks that bombard users with approval prompts until one is accepted. More advanced adversary-in-the-middle (AiTM) phishing kits proxy the connection between the user and the real service, capturing both the password and the time-based code. These techniques can bypass most mainstream MFA methods except the strongest phishing-resistant options, particularly modern hardware security keys. The security profile of specific MFA methods varies significantly. SMS-based one-time passwords remain widely deployed due to their simplicity and user familiarity. However, telecom infrastructure was not designed with modern threat models in mind. SMS codes are vulnerable to interception via SIM swapping, protocol weaknesses, and phishing, because users can be tricked into entering both their password and the code into a fake website. Authenticator applications using time-based one-time passwords (TOTP) provide stronger protection by generating codes locally and avoiding telecom networks. Compared to SMS codes, authenticator apps significantly reduce exposure to telecom-level attacks, though both remain phishable and vulnerable to AiTM techniques. Usability challenges also appear, such as device loss or migration issues. Push-based MFA improves usability by enabling users to approve logins with a tap instead of transcribing a code. While this reduces friction and can increase adoption, it introduces behavioural risks. Attackers increasingly exploit MFA fatigue attacks, relying on users becoming desensitised to repeated prompts. In these cases, the limitation is not the technology but human behaviour. If users develop poor “prompt hygiene”—approving notifications without scrutiny—the value of the second factor diminishes. Hardware security keys represent the strongest tier of MFA. They use public key cryptography and bind authentication to the legitimate website or application, making them resistant to phishing and adversary-in-the-middle attacks. Even if a user interacts with a convincing fake login page, the hardware key refuses to authenticate because the domain does not match the registered origin. This makes hardware keys the most effective option for high-risk accounts. Biometrics also offer convenience and strong assurance but raise privacy concerns and difficulties related to revocation, since biometric traits cannot be replaced. A clearer tiered model therefore emerges: SMS codes at the weaker end, authenticator apps and push MFA in the moderate middle, and hardware keys and hardware-bound biometrics at the strong, phishing-resistant end. Yet even strong MFA can be undermined by organisational decisions. Real-world breaches often occur not because MFA is inherently weak, but because it is optional instead of mandatory, inconsistently applied across systems, or weakened by insecure fallback methods such as SMS recovery codes or email resets. Moreover, if high-assurance authentication is implemented but users revert to insecure recovery processes, the overall posture weakens. The effectiveness of MFA therefore depends not only on the chosen method but also on configuration, enforcement, and policy design. These issues highlight the crucial role of human factors. Organisations must ensure users understand phishing risks, MFA fatigue, and the importance of verifying prompt legitimacy. A technically strong MFA system can still fail if users habitually approve unexpected requests. Security culture, training, and clear communication are therefore essential to unlocking the full value of MFA. To address these challenges, MFA must operate within a broader cybersecurity strategy rather than as a standalone control. Under a zero-trust model, no user or device is trusted by default. Authentication and authorisation decisions rely on multiple continuous signals, including identity, device posture, behavioural analytics, and contextual risk. MFA contributes one signal within this ecosystem but does not prevent lateral movement once an attacker is already inside the network. Complementary measures—such as conditional access policies, privileged access management (PAM), network segmentation, and continuous monitoring—are required to form a cohesive identity-centric defence. Adaptive or risk-based MFA further enhances integration by tailoring authentication requirements to the context of each login attempt. By stepping up security only when behaviour appears unusual (for example, from an unknown location or unmanaged device), adaptive MFA reduces fatigue and improves usability without lowering security expectations. This reflects modern identity frameworks such as NIST SP 800-63, which emphasise assurance levels appropriate to risk. Likewise, regulatory and industry frameworks such as PCI DSS and ISO 27001 require MFA for sensitive environments, demonstrating that MFA’s role is not only technical but also compliance-driven. Looking forward, MFA also plays a transitional role in the shift toward passwordless authentication, where hardware-bound credentials and biometrics replace passwords entirely. As organisations adopt these approaches, MFA becomes part of a broader movement away from passwords as the primary security mechanism. In conclusion, multi-factor authentication is a highly effective control for reducing the risk of data breaches caused by compromised credentials. By requiring additional proof of identity beyond a password, MFA blocks many common attack paths and forces attackers to invest more effort and resources. However, its impact depends on method selection, configuration, user behaviour, and organisational policy. When embedded within a wider defence-in-depth strategy, supported by standards and complemented by continuous risk assessment, MFA becomes a central component of modern cybersecurity. Careful implementation, consistent enforcement, and strong human-centred design ensure that MFA delivers its intended protection against increasingly sophisticated credential-based attacks. \n",
            "\n",
            "Feedback:  EDUCATOR FEEDBACK & ACTIONABLE INSIGHTS\n",
            "The student demonstrates a strong learning process by actively seeking to understand the nuances of MFA, as evidenced by their detailed prompts exploring various methods, their trade-offs, and integration into broader strategies. The engagement level is high, consistently probing for deeper insights. To further enhance their learning, the student could explore specific case studies of data breaches where MFA was either effective or bypassed, and investigate the evolving landscape of MFA bypass techniques and countermeasures. This would deepen their critical evaluation and real-world application.\n",
            "\n",
            "RUBRIC SCORES\n",
            "Conceptual Understanding: 10/10 - The student's prompts show a comprehensive grasp of MFA, its components, and its role in cybersecurity, demonstrating deep theoretical understanding.\n",
            "Application to Real-World Scenarios: 9/10 - The student effectively prompts for comparisons of MFA methods and their integration into strategies, indicating a strong ability to apply concepts to practical scenarios.\n",
            "Critical Evaluation: 10/10 - The student consistently pushes for an analysis of trade-offs, limitations, and evolving attack vectors, showcasing excellent critical thinking and evaluation skills.\n",
            "Structure and Academic Writing: 10/10 - The student's prompts are clear, well-articulated, and directly address the assignment's requirements, indicating strong organizational and communication skills. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the meta-rubric for the llm judge\n",
        "\n",
        "META_RUBRIC = \"\"\"\n",
        "ou must evaluate the **quality** of the feedback produced by an LLM.\n",
        "\n",
        "Evaluate the feedback only using the following meta-rubric:\n",
        "\n",
        "### META-RUBRIC\n",
        "\n",
        "1. **Accuracy**\n",
        "   Does the feedback correctly reflect the student’s submission?\n",
        "\n",
        "2. **Specificity**\n",
        "   Is the feedback concrete, detailed, and supported with examples rather than vague advice?\n",
        "\n",
        "3. **Constructiveness**\n",
        "   Does the feedback guide the student on how to improve?\n",
        "\n",
        "4. **Alignment with assignment rubric**\n",
        "   Does the feedback clearly reference (and stay aligned with) the criteria used to judge the work?\n",
        "\n",
        "5. **Tone and clarity**\n",
        "   Is the feedback easy to understand, supportive, and professionally written?\n",
        "\n",
        "### SCORING INSTRUCTIONS\n",
        "For each dimension, give a score from **0 to 10** and provide a justification.\n",
        "Then provide an overall score and a summary.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Lg_sE24mbuIx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define a sub-model for a single rubric criterion\n",
        "# class CriterionResult(BaseModel):\n",
        "#     score: int = Field(..., description=\"The score (0-10) for this dimension.\")\n",
        "#     explanation: str = Field(..., description=\"The justification for the score.\")"
      ],
      "metadata": {
        "id": "5LU-bidVBv35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the main output model with explicit fields for each criterion\n",
        "# class JudgeOutput(BaseModel):\n",
        "#     accuracy: CriterionResult\n",
        "#     specificity: CriterionResult\n",
        "#     constructiveness: CriterionResult\n",
        "#     alignment: CriterionResult\n",
        "#     tone_clarity: CriterionResult\n",
        "\n",
        "#     overall_score: float = Field(\n",
        "#         ...,\n",
        "#         description=\"The average of the five scores, rounded to one decimal place.\"\n",
        "#     )\n",
        "#     summary: str = Field(\n",
        "#         ...,\n",
        "#         description=\"A short summary of the feedback's strengths and weaknesses.\"\n",
        "#     )"
      ],
      "metadata": {
        "id": "40LNiT7J9aWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # create the llm-as-judge evaluator\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "# print(\"API Key set:\", os.environ.get('OPENAI_API_KEY') is not None)\n",
        "\n",
        "# # judge = Agent(model=\"gpt-5-mini, retries=3\")\n",
        "# judge = Agent(model=\"gpt-5-mini\")\n",
        "\n",
        "# class MetaRubricEvaluator(Evaluator):\n",
        "\n",
        "#     async def evaluate(self, case):\n",
        "#         prompt = f\"\"\"\n",
        "# You are an expert academic evaluator.\n",
        "\n",
        "# The student's submission is:\n",
        "# --------------------\n",
        "# {student_submission}\n",
        "# --------------------\n",
        "\n",
        "# The feedback you must evaluate is:\n",
        "# --------------------\n",
        "# {llm_feedback}\n",
        "# --------------------\n",
        "\n",
        "# Use the following meta-rubric:\n",
        "# {META_RUBRIC}\n",
        "\n",
        "# Now score the feedback strictly according to the meta-rubric.\n",
        "# \"\"\"\n",
        "#         result = await judge.run(\n",
        "#             prompt,\n",
        "#             output_type=JudgeOutput # Pass the schema for structured output\n",
        "#         )\n",
        "#         # return EvaluationResult(model_output=result.output)\n",
        "#         return EvaluationResult(\n",
        "#             name=\"Meta_Rubric_Score\", # Required field\n",
        "#             value=result.output.overall_score, # Extract the scalar score\n",
        "#             reason=result.output.summary, # Extract the summary as the reason\n",
        "#             source=judge.model.model_name\n",
        "#         )"
      ],
      "metadata": {
        "id": "ZE6BBbAHdCp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92cdec26-5fd2-4f71-ac4d-ee9477dc6b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key set: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# define the G-Eval metric using meta rubrci defined\n",
        "meta_evaluator = GEval(\n",
        "    name=\"Feedback Quality\",\n",
        "    criteria=META_RUBRIC,\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,          # submission\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT,  # LLM generated feedback\n",
        "        LLMTestCaseParams.CONTEXT         # original feedback rubrick\n",
        "    ],\n",
        "    model=\"gpt-4o\"  # G-Eval works best with GPT-4 level models\n",
        "    # model=\"gpt-5-mini\"\n",
        ")"
      ],
      "metadata": {
        "id": "teQuzMdUTRoy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # run the evaluation\n",
        "\n",
        "# evaluator = MetaRubricEvaluator()\n",
        "\n",
        "# case = {\"id\": \"case1\"}\n",
        "# result = await evaluator.evaluate(case)\n",
        "# result\n"
      ],
      "metadata": {
        "id": "8hN80Z6lhZ2D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # print full result\n",
        "# # print(\"\\nFull Result Object:\")\n",
        "# # print(result)\n",
        "\n",
        "# # pring score and reason\n",
        "# print(f\"Score: {result.value}\")\n",
        "# print(\"Reason:\")\n",
        "# result.reason\n"
      ],
      "metadata": {
        "id": "Cb5T73aMEz5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5a531603-c525-4c6a-f240-28dce1899e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 8.4\n",
            "Reason:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Strengths: The feedback is positive, aligned with the rubric, and offers a clear, actionable next step (investigate case studies and bypass techniques). It also scores the student's performance across appropriate categories and maintains a supportive tone. Weaknesses: The feedback occasionally mislabels the submission as 'prompts' instead of an essay, and it is light on concrete examples and specific resources or revision tasks. To improve, the educator should cite brief examples from the submission to justify each rubric score and recommend specific case studies or readings the student could examine.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the test case\n",
        "\n",
        "# input = Student's work\n",
        "# actual_output = The feedback we are grading\n",
        "# context = The original rubric (reference material)\n",
        "test_case = LLMTestCase(\n",
        "    input=student_submission,\n",
        "    actual_output=llm_feedback,\n",
        "    context=[str(rubric)]\n",
        ")\n",
        "\n",
        "# run the G-Eval\n",
        "meta_evaluator.measure(test_case)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91,
          "referenced_widgets": [
            "5c08a862406b49b89ee544abfe86b80e",
            "dccd97c43f24438b97f5eb6d6086ec1f"
          ]
        },
        "id": "gR6GXdGdTy6_",
        "outputId": "39071ae9-2625-42c6-c2ec-2e7e982d5d25"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c08a862406b49b89ee544abfe86b80e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G-Eval Score: 0.8234250424415979\n",
            "Reasoning:\n",
            "The feedback accurately reflects the student's comprehensive understanding of MFA, aligning well with the assignment rubric's criteria for conceptual understanding and critical evaluation. It provides specific suggestions for improvement, such as exploring case studies, which enhances its constructiveness. However, the feedback could be more specific by referencing particular sections of the student's work to improve specificity. The tone is supportive and clear, maintaining professional clarity throughout.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"G-Eval Score: {meta_evaluator.score:.2f}\")\n",
        "print(\"Reason\")\n",
        "meta_evaluator.reason"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "NvZjj1EDWMDG",
        "outputId": "419821a2-61ff-4235-c9e4-69586da70795"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G-Eval Score: 0.82\n",
            "Reason\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The feedback accurately reflects the student's comprehensive understanding of MFA, aligning well with the assignment rubric's criteria for conceptual understanding and critical evaluation. It provides specific suggestions for improvement, such as exploring case studies, which enhances its constructiveness. However, the feedback could be more specific by referencing particular sections of the student's work to improve specificity. The tone is supportive and clear, maintaining professional clarity throughout.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_evaluator.evaluation_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd6yMu1HWHMf",
        "outputId": "3a6584df-f39a-493a-9aba-9d9a728ce09e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Evaluate the Accuracy by comparing the feedback to the student's submission to ensure it correctly reflects the content and intent of the student's work.\",\n",
              " 'Assess the Specificity by checking if the feedback includes concrete details and examples, rather than vague or general advice.',\n",
              " 'Examine the Constructiveness by determining if the feedback provides clear guidance on how the student can improve their work.',\n",
              " 'Check the Alignment with assignment rubric by ensuring the feedback references and adheres to the criteria outlined in the assignment rubric.',\n",
              " 'Review the Tone and clarity by evaluating if the feedback is easy to understand, supportive, and professionally written.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}